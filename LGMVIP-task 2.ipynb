{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Level Task 1\n",
    "# Develop A Neural Network That Can Read Handwriting\n",
    "\n",
    "Begin your neural network machine learning project with the MNIST Handwritten Digit Classification Challenge and using Tensorflow and CNN. It has a very user-friendly interface thatâ€™s ideal for beginners. Dataset can be seen on MNIST or can find here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Note: \"%matplotlib inline\" is used in order to allow the images to be displayed inside the notebook otherwise it would display a dump of the images location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the MNIST dataset from the tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes of the traning and testing arrays\n",
    "\n",
    "We can see that the labels are rank one arrays whereas the feature vectors are 3-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 28, 28)\n",
      "Shape of Y_train: (60000,)\n",
      "Shape of X_test: (10000, 28, 28)\n",
      "Shape of Y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of Y_train: {Y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying a sample\n",
    "\n",
    "We have taken the sample at index number 510 and have displayed the corresponsing image below. The label of the image confirms that it is indeed '1' in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of image:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a71ed88cd0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_number = 510\n",
    "print(\"Label of image: \", Y_train[image_number])\n",
    "plt.imshow(X_train[image_number], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping the feature vectors\n",
    "\n",
    "We need to reshape the arrays as the tensorflow keras API required 4-dimensional NumPy arrays and the arrays that we have are 3-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing feature types\n",
    "After we have successfully reshaped the features we now need to convert the data into float as we are going to be normalizing the data and we need to be able to obtain values in decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data\n",
    "After the conversion of the features into float we now need to normalize our vectors as it will allow our neural network to converge faster and work more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the shapes after transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_train has 60000 images\n",
      "X_test has 10000 images\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(f\"X_train has {X_train.shape[0]} images\")\n",
    "print(f\"X_test has {X_test.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Convolutional Neural Network(CNN) Model\n",
    "\n",
    "We are going to be building a sequential so we will save the sequential model into the variable \"cnn\".\n",
    "We are going to be using the following layers to build our Convolutional Neural Network:\n",
    "1. Convolutional Layer (Conv2D): Concolutional Layer performs the convolution operation which is used to extract the feature maps\n",
    "2. MaxPooling Layer: Max Pooling allows us to reduce the spatial size of the image by disregarding the unnecessary features\n",
    "3. Flatten Layer: This layer just unravels the data from a 2D array to a 1D array\n",
    "4. Dense Layer: This is a fully connected layer in which all the hidden units in one layer are connected to all the hidden units in the next layer\n",
    "5. Dropout Layer: It allows us to prevent iverfitting by removing output from some of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(28, kernel_size=(4,4), input_shape=image_shape))\n",
    "cnn.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=tf.nn.relu))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the Neural Network\n",
    "\n",
    "We have used \"adam\" optimizer as it performs back-propagation very efficiently and converges pretty quickly, we have also defined the loss function which is \"sparse_categorical_crossentropy\" and we have asked for \"accuracy\" as a scoring metric.\n",
    "\n",
    "**Note: Not showing the training process due to time constraints.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 42s 21ms/step - loss: 0.3707 - accuracy: 0.8914\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0688 - accuracy: 0.9796\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0461 - accuracy: 0.9860\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0353 - accuracy: 0.9890\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0245 - accuracy: 0.9919\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0228 - accuracy: 0.99257s - loss: 0.0223 - accuracy: 0.99 - ETA: 7s - loss: 0.0223 - accuracy: 0.99 - ETA: 7s - l - E\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 37s 19ms/step - loss: 0.0197 - accuracy: 0.99320s - los\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0187 - accuracy: 0.9934\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0153 - accuracy: 0.9944\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0134 - accuracy: 0.9954\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0136 - accuracy: 0.99500s - l\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.0109 - accuracy: 0.9960\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0123 - accuracy: 0.9957\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0114 - accuracy: 0.9961\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.0101 - accuracy: 0.99702s - loss: 0.0100 - ac - ETA: 1s - loss: 0.0100 - ac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a71900ac70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "cnn.fit(X_train, Y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0615 - accuracy: 0.9835\n",
      "Accuracy: 98.35000038146973%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f\"Accuracy: {accuracy[1]*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying image along with its predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_number = 1100\n",
    "plt.imshow(X_test[image_number].reshape(28, 28),cmap='Greys')\n",
    "Y_pred = cnn.predict(X_test[image_number].reshape(1, 28, 28, 1))\n",
    "print(f\"Predicted Label: {Y_pred.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
